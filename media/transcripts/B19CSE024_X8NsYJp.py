# -*- coding: utf-8 -*-
"""LAB_8_B19CSE024.ipnyb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fk9YoHWidXq3KK2Ai9JujXOWb7vD-wuK
"""

import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import GaussianNB

drive.mount('/content/drive')
iris = pd.read_csv('/content/drive/My Drive/data/Iris.csv')
data_replace={'Species':{'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2}}
iris=iris.replace(data_replace)
iris.head()

X = iris.iloc[:,1:5]
Y = iris.iloc[:,-1]
from sklearn.model_selection import train_test_split
xtn, xts, ytn, yts = train_test_split(X, Y, test_size = 0.2)

corr = X.corr()
sns.heatmap(corr, annot=True)
plt.show()

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
sdt = ss.fit_transform(X) 
#standardized data
sdt[0:5]

import numpy as np
cvm = np.cov(sdt.T)
print(cvm)
print("---------------------------------------------")
evals, evecs = np.linalg.eig(cvm)
print("Eigen values: ", evals)
print("---------------------------------------------")
print("Eigen Vectors: \n", evecs)

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
cps = pca.fit_transform(X, Y)
print(cps[0:5])

var1 = pca.explained_variance_ratio_[0]
var2 = pca.explained_variance_ratio_[1]
print("preserves %d information" %((var1 + var2)*100))

plt.xlabel('PCA_one')
plt.ylabel('PCA_two')
plt.scatter(
   cps[:,0],
   cps[:,1],
   c = Y,
   cmap='rainbow',
   alpha=0.7,
   edgecolors='b')

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LD
ldam = LD(n_components=2)
lda = ldam.fit_transform(X, Y)
print(lda[0:5])

var1_lda = ldam.explained_variance_ratio_[0]
var2_lda = ldam.explained_variance_ratio_[1]
print("preserves %d information" %((var1_lda + var2_lda)*100))

plt.xlabel('LDA_one')
plt.ylabel('LDA_two')
plt.scatter(
  lda[:,0],
  lda[:,1],
  c=Y,
  cmap='rainbow',
  alpha=0.7,
  edgecolors='b'
)

xtn = ss.fit_transform(xtn)
xts = ss.transform(xts)
ganb = GaussianNB()
ganb.fit(xtn, ytn)
ypd = ganb.predict(xts) 
print ("Accuracy obtained : ", accuracy_score(yts, ypd)*100)

X_pca = cps[:,:4]
Y_pca = iris['Species'].values
xtn_pca, xts_pca, ytn_pca, yts_pca = train_test_split(X_pca, Y_pca, test_size = 0.2)
xtn_pca = ss.fit_transform(xtn_pca)
xts_pca = ss.transform(xts_pca)
ganb_pca = GaussianNB()
ganb_pca.fit(xtn_pca, ytn_pca)
ypd_pca = ganb_pca.predict(xts_pca) 
print ("Accuracy obtained : ", accuracy_score(yts_pca, ypd_pca)*100)

X_lda = lda[:,:4]
Y_lda = iris['Species'].values
xtn_lda, xts_lda, ytn_lda, yts_lda = train_test_split(X_lda, Y_lda, test_size = 0.2)
xtn_lda = ss.fit_transform(xtn_lda)
xts_lda = ss.transform(xts_lda)
ganb_lda = GaussianNB()
ganb_lda.fit(xtn_lda, ytn_lda)
ypd_lda = ganb_lda.predict(xts_lda) 
print ("Accuracy obtained : ", accuracy_score(yts_lda, ypd_lda)*100)

"""3. Feature selection"""

ddf = pd.read_csv('/content/drive/My Drive/data/diabetes.csv')
print(ddf.head())
X_ddf = ddf[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]
Y_ddf = ddf[['Outcome']]

"""Visulaization of data (targets)"""

ddf['Outcome'].value_counts().plot(kind='barh')

import seaborn as sns
c = ddf.corr()
sns.heatmap(c, annot=True)
plt.show()

ddf.isna().sum()
plt.figure(figsize=(12,7))
sns.heatmap(ddf.isnull(), cbar=False, cmap='coolwarm', yticklabels=False)
plt.title('Null values');

from sklearn.preprocessing import MinMaxScaler as MMS
from sklearn.preprocessing import StandardScaler as SSC
from sklearn.model_selection import train_test_split
xtn_f, xts_f, ytn_f, yts_f = train_test_split(X_ddf, Y_ddf, test_size=0.2, stratify = Y_ddf, shuffle=True)
mims = MMS()
xtn_mims = mims.fit_transform(xtn_f)
xts_mims = mims.transform(xts_f)

ssc = SSC()
xtn_ssc = ssc.fit_transform(xtn_f)
xts_ssc = ssc.transform(xts)

acc_orig = []
f1_orig = []
def rfe(n_features):
  estimator = SVC(kernel="linear")
  features_ = []
  selector = RFE(estimator, n_features_to_select=n_features, step=1)
  selector = selector.fit(X_train_,Y_train_.values.ravel() )
  #print(selector_rfe_mm.support_)
  #print(selector_rfe_mm.ranking_)
  for i in range(8):
    if(selector.support_[i]):
      features_.append(X_ddf[i])
  #features_mm_rfe
  data_tr = X_train_[features_]
  data_te = X_test_[features_] 

  estimator.fit(data_tr,Y_train_.values.ravel())
  print("Number of features = ",n_features)
  print('Accuracy = ',estimator.score(data_te,Y_test_.values.ravel()))
  acc_orig.append(estimator.score(data_te,Y_test_.values.ravel()))
  f1_orig.append(f1_score(Y_test_.values.ravel(),estimator.predict(data_te) , average='macro'))
  print("Features = ",features_)
  print("F1 Score = ",f1_orig[-1])

print("Unscaled Data")
print()
for i in range(3,7):
  rfe(i)
  print()